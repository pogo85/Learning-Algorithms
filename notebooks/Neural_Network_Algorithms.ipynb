{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Network_Algorithms.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR5e6zZ1JWCC"
      },
      "source": [
        "import os \n",
        "import glob \n",
        "import PIL\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "from random import randrange\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from sklearn import preprocessing\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Input, Dense, Activation, Flatten, Conv1D, Dropout,MaxPooling1D,MaxPool1D\n",
        "from keras.optimizers import SGD,Adam\n",
        "from scipy.spatial import distance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Retrieving data \n",
        "data55 = pd.read_excel(\"/content/drive/My Drive/Neural_Nets_Assignment_2/data55.xlsx\", header=None)\n",
        "data55 = data55.sample(frac=1) # Shuffling\n",
        "\n",
        "data5 = sio.loadmat('/content/drive/My Drive/Neural_Nets_Assignment_2/data5.mat')\n",
        "data5 = np.array(data5['x']) # Extracting array from dict\n",
        "np.random.shuffle(data5) # Shuffling\n",
        "\n",
        "input = sio.loadmat('/content/drive/MyDrive/Neural_Nets_Assignment_2/input.mat')\n",
        "input = np.array(input['x'])\n",
        "input = input.flatten()\n",
        "input = np.dstack(input)\n",
        "input = input.T\n",
        "\n",
        "class_label = sio.loadmat('/content/drive/MyDrive/Neural_Nets_Assignment_2/class_label.mat')\n",
        "class_label = np.array(class_label['y'])\n",
        "class_label = np.array([x - 1 for x in class_label])\n",
        "\n",
        "# Shuffling input and class_label together\n",
        "rng_state = np.random.get_state()\n",
        "np.random.shuffle(input)\n",
        "np.random.set_state(rng_state)\n",
        "np.random.shuffle(class_label)\n",
        "\n",
        "# Splits data using hold out cross validation\n",
        "train_data55, validate_data55, test_data55 = np.split(data55, [int(.7*len(data55)), int(.8*len(data55))]) \n",
        "train_data5, validate_data5, test_data5 = np.split(data5, [int(.7*len(data5)), int(.8*len(data5))])\n",
        "\n",
        "# Split data5 into x and y columns\n",
        "train_data5_x = train_data5[0:, 0:-1]\n",
        "train_data5_y = train_data5[0:, -1]\n",
        "test_data5_x = test_data5[0:, 0:-1]\n",
        "test_data5_y = test_data5[0:, -1]\n",
        "validate_data5_x = validate_data5[0:, 0:-1]\n",
        "validate_data5_y = validate_data5[0:, -1]\n",
        "\n",
        "train_input, validate_input, test_input = np.split(input, [int(.7*len(input)), int(.8*len(input))])\n",
        "train_class_label, validate_class_label, test_class_label = np.split(class_label, [int(.7*len(class_label)), int(.8*len(input))])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwBT1UgmKlTT",
        "cellView": "code"
      },
      "source": [
        "#@title Functions and classes used in the questions\n",
        "\n",
        "class multilayer_Perceptron:\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        neurons1 = 5\n",
        "        neurons2 = 28\n",
        "        self.lr = 1\n",
        "        ip_dim = x.shape[1]\n",
        "        op_dim = y.shape[1]\n",
        "\n",
        "        self.w1 = np.random.randn(ip_dim, neurons1)\n",
        "        self.b1 = np.zeros((1, neurons1))\n",
        "        self.w2 = np.random.randn(neurons1, neurons2)\n",
        "        self.b2 = np.zeros((1, neurons2))\n",
        "        self.w3 = np.random.randn(neurons2, op_dim)\n",
        "        self.b3 = np.zeros((1, op_dim))\n",
        "        self.y = y\n",
        "\n",
        "    def feedforward(self):\n",
        "        z1 = np.dot(self.x, self.w1) + self.b1\n",
        "        self.a1 = sigmoid(z1)\n",
        "        z2 = np.dot(self.a1, self.w2) + self.b2\n",
        "        self.a2 = sigmoid(z2)\n",
        "        z3 = np.dot(self.a2, self.w3) + self.b3\n",
        "        self.a3 = sigmoid(z3)\n",
        "        \n",
        "    def backprop(self):\n",
        "        loss = error(self.a3, self.y)\n",
        "        a3_delta = cross_entropy(self.a3, self.y) \n",
        "        z2_delta = np.dot(a3_delta, self.w3.T)\n",
        "        a2_delta = z2_delta * sigmoid_derv(self.a2) \n",
        "        z1_delta = np.dot(a2_delta, self.w2.T)\n",
        "        a1_delta = z1_delta * sigmoid_derv(self.a1)\n",
        "\n",
        "        self.w3 -= self.lr * np.dot(self.a2.T, a3_delta)\n",
        "        self.b3 -= self.lr * np.sum(a3_delta, axis=0, keepdims=True)\n",
        "        self.w2 -= self.lr * np.dot(self.a1.T, a2_delta)\n",
        "        self.b2 -= self.lr * np.sum(a2_delta, axis=0)\n",
        "        self.w1 -= self.lr * np.dot(self.x.T, a1_delta)\n",
        "        self.b1 -= self.lr * np.sum(a1_delta, axis=0)\n",
        "\n",
        "    def predict(self, data):\n",
        "        self.x = data\n",
        "        self.feedforward()\n",
        "        return (self.a3>0.5)*1\n",
        "\t\t\t\n",
        "def stat_measures(confusion):\n",
        "    accuracy = (confusion[0][0] + confusion[1][1]) / (np.sum(confusion))\n",
        "    sensitivity = confusion[0][0] / (confusion[1][1] + confusion[1][0])\n",
        "    specificity = confusion[1][1] / (confusion[1][1] + confusion[0][1])\n",
        "    print(\"Accuracy is %d\" % accuracy)\n",
        "    print(\"Specificity is %d\" % specificity)\n",
        "    print(\"Sensitivity is %d\" % sensitivity)\n",
        "\n",
        "def hypothesis(a):\n",
        "    #if -a > np.log(np.finfo(type(a)).max):\n",
        "        #return 0.0    \n",
        "    return (1. / (1. + np.exp(-a)))\n",
        "\n",
        "def cross_validation_split(dataset, folds=5):\n",
        "\tdataset_split = list()\n",
        "\tdataset_copy = list(dataset)\n",
        "\tfold_size = int(len(dataset) / folds)\n",
        "\tfor i in range(folds):\n",
        "\t\tfold = list()\n",
        "\t\twhile len(fold) < fold_size:\n",
        "\t\t\tindex = randrange(len(dataset_copy))\n",
        "\t\t\tfold.append(dataset_copy.pop(index))\n",
        "\t\tdataset_split.append(fold)\n",
        "\treturn dataset_split\n",
        "\n",
        "def polynomial_kernel_function(u, v, d):\n",
        "    return (np.dot(u, v) + 1) ** d\n",
        "\n",
        "def rbf_kernel_function(u, v, s=1):\n",
        "    return np.exp(-np.linalg.norm(u - v) / (2 * (s ** 2)))\n",
        "\n",
        "def gaussian_function(x1, x2, sigma=1):\n",
        "  return np.exp(- (np.linalg.norm(x1 - x2, 2)) ** 2 / (2 * sigma ** 2))\n",
        "\n",
        "def create_one_hot_encoding(classes, shape):\n",
        "    one_hot_encoding = np.zeros(shape)\n",
        "    for i in range(0, len(one_hot_encoding)):\n",
        "        one_hot_encoding[i][int(classes[i])] = 1\n",
        "    return one_hot_encoding\n",
        "\n",
        "def kmeans(X,k,max_iterations=100):\n",
        "    if isinstance(X, pd.DataFrame):X = X.values\n",
        "    idx = np.random.choice(len(X), k, replace=False)\n",
        "    centroids = X[idx, :]\n",
        "    P = np.argmin(distance.cdist(X, centroids, 'euclidean'),axis=1)\n",
        "    for _ in range(max_iterations):\n",
        "        centroids = np.vstack([X[P==i,:].mean(axis=0) for i in range(k)])\n",
        "        tmp = np.argmin(distance.cdist(X, centroids, 'euclidean'),axis=1)\n",
        "        if np.array_equal(P,tmp):break\n",
        "        P = tmp\n",
        "    return centroids\n",
        "\n",
        "def kfold(z,i):\n",
        "    data=z.copy()\n",
        "    test=z[i]\n",
        "    del data[i]\n",
        "    train=np.concatenate(data)\n",
        "    return train,test\n",
        "\n",
        "def l_h_comp(C, alpha_prime_j, alpha_prime_i, y_j, y_i):\n",
        "        if(y_i != y_j):\n",
        "            return (max(0, alpha_prime_j - alpha_prime_i), min(C, C - alpha_prime_i + alpha_prime_j))\n",
        "        else:\n",
        "            return (max(0, alpha_prime_i + alpha_prime_j - C), min(C, alpha_prime_i + alpha_prime_j))\n",
        "\n",
        "def hypothesis_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def backpropogation1(w1, X_vector, w2, hidden_layer1, w3, hidden_layer2, output_weights, hidden_layer3, y_actual, class_label, learning_rate):\n",
        "    \n",
        "    input_vector_with_bias = np.hstack((1, X_vector))\n",
        "    gradient = (((((((((-2*(y_actual-class_label)*hypothesis_derivative(class_label)*output_weights[1:\n",
        "]*hypothesis_derivative(hidden_layer3[:,np.newaxis])).T.dot(w3[1:].T)).T)*hypothesis_derivative(hidden_layer2[:,np.newaxis])).T).dot(w2[1:].T)).T)*hypothesis_derivative(hidden_layer1[:,np.newaxis])).dot(\n",
        "input_vector_with_bias[:,np.newaxis].T))\n",
        "    w1 = w1-learning_rate*gradient.T\n",
        "    return w1\n",
        "\n",
        "def backpropogation2(w2, hidden_layer1, w3, hidden_layer2, output_weights, hidden_layer3, y_actual, class_label, learning_rate):\n",
        "        \n",
        "    hidden_layer_with_bias = np.hstack((1, hidden_layer1))\n",
        "    gradient = ((((-2*(y_actual-class_label)*hypothesis_derivative(class_label)*output_weights[1:]*hypothesis_derivative(hidden_layer3[:,np.newaxis])).T.dot(w3[1:].T)).T)*hypothesis_derivative(hidden_layer2[:,np.newaxis])).dot(hidden_layer_with_bias[:,np.newaxis].T)\n",
        "    w2 = w2-learning_rate*gradient.T\n",
        "    return w2\n",
        "\n",
        "def backpropogation3(w2, hidden_layer1, output_weights, hidden_layer2, y_actual, class_label, learning_rate):\n",
        "    hidden_layer1_with_bias = np.hstack((1, hidden_layer1))\n",
        "    gradient = (-2*(y_actual-class_label)*\n",
        "                hypothesis_derivative(class_label)*\n",
        "                output_weights[1:]*\n",
        "                hypothesis_derivative(hidden_layer2[:,np.newaxis])).dot(hidden_layer1_with_bias[:,np.newaxis].T\n",
        "                )\n",
        "    w2=w2-learning_rate*gradient.T\n",
        "    return w2\n",
        "\n",
        "def backpropogation4(output_weights, hidden_layer2, y_actual, class_label, learning_rate):\n",
        "    hidden_layer2_with_bias = np.hstack((1, hidden_layer2))\n",
        "    gradient = -2*(y_actual-class_label)*hypothesis_derivative(class_label)*hidden_layer2_with_bias\n",
        "\n",
        "    output_weights = output_weights-learning_rate*gradient[:,np.newaxis]\n",
        "    \n",
        "    return output_weights\n",
        "\n",
        "def forward_propagation(weight_matrix, feature_vector):\n",
        "    feature_matrix = np.hstack((1, feature_vector))\n",
        "    return hypothesis(weight_matrix.T.dot(feature_matrix))\n",
        "\n",
        "def sigmoid(s):\n",
        "    return 1/(1 + np.exp(-s))\n",
        "\n",
        "def sigmoid_derv(s):\n",
        "    return s * (1 - s)\n",
        "\n",
        "def cross_entropy(pred, real):\n",
        "    n_samples = real.shape[0]\n",
        "    res = pred - real\n",
        "    return res/n_samples\n",
        "\n",
        "def error(pred, real):\n",
        "    n_samples = real.shape[0]\n",
        "    #logp = - np.log(pred[np.arange(n_samples), real.argmax(axis=1)])\n",
        "    loss = np.sum(real-pred)/n_samples\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZedTQDkc-jhS",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0abe2e99-dbd9-4497-b8d5-cf07d8891b2b"
      },
      "source": [
        "#@title Implement non-linear perceptron algorithm for the classification. The dataset (data55.mat) contains 4 features and the last column is the output (class label). You can use hold-out cross- validation (70, 10, and 20%) for the selection of training, validation and test instances. Evaluate accuracy, sensitivity and specificity measures for the evaluation of test instances. (Packages such as keras, tensorflow, pytorch for python and MATLAB deep learning toolbox etc. are not allowed)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    weights = [0] * (len(data55.iloc[0]))\n",
        "    weights[-1] = 1 # extra weight is taken as bias (initialized to 1)\n",
        "    iterations = 1000\n",
        "    alpha = 0.05\n",
        "    \n",
        "    # Training\n",
        "    for i in range(iterations):\n",
        "        gradient = [0] * (len(train_data55.iloc[0]))\n",
        "        for j in range(len(train_data55[0])):\n",
        "            activation = np.dot(weights, train_data55.iloc[j]) + weights[-1]\n",
        "            for k in range(len(weights) - 1):\n",
        "                gradient[k] += ((hypothesis(activation) - train_data55.iat[j, 4]) * hypothesis(activation) * (1 - hypothesis(activation)) * train_data55.iat[j, k])\n",
        "            gradient[-1] = (hypothesis(activation) - train_data55.iat[j, 4]) * (hypothesis(activation)) * (1 - hypothesis(activation))\n",
        "        for k in range(len(weights)):\n",
        "            weights[k] = weights[k] - alpha * gradient[k]\n",
        "    print(\"Weights are %s\" % weights)\n",
        "    \n",
        "    # Validation\n",
        "    error = 0\n",
        "    for j in range(len(validate_data55[0])):\n",
        "        activation = np.dot(weights, validate_data55.iloc[j]) + weights[-1]\n",
        "        if (validate_data55.iat[j, 4] != int(np.round(hypothesis(activation)))):\n",
        "            error += 1\n",
        "    print(\"Error in validation set is %d\" % (error / len(validate_data55[0])))\n",
        "    \n",
        "    # Testing\n",
        "    confusion_matrix = [[0, 0], [0, 0]]\n",
        "    for k in range(len(test_data55[0])):\n",
        "        activation = np.dot(weights, test_data55.iloc[k]) + weights[-1]\n",
        "        confusion_matrix[test_data55.iat[k, 4]][int(np.round(hypothesis(activation)))] += 1\n",
        "    print(\"Confusion matrix is %s\" % confusion_matrix)\n",
        "    stat_measures(confusion_matrix)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights are [-0.7407271470402489, -2.030111120240188, 2.7909703049418635, 1.2580134100506142, 1.055901891815501]\n",
            "Error in validation set is 0\n",
            "Confusion matrix is [[11, 0], [0, 9]]\n",
            "Accuracy is 1\n",
            "Specificity is 1\n",
            "Sensitivity is 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "id": "g-bRe36HeVnH",
        "outputId": "443a7992-3e02-4499-f6bb-aa6461b0cdbf"
      },
      "source": [
        "#@title The dataset (data5.mat) contains 72 features and the last column is the output (class labels). Design a multilayer perceptron based neural network (two hidden layers) for the classification. You can use both holdout (70, 10, and 20%) and 5-fold cross-validation approaches for evaluating the performance of the classifier (individual accuracy and overall accuracy). You can select the number of hidden neurons of each hidden layer and other MLP parameters using grid-search method. (Packages such as keras, tensorflow, pytorch for python and MATLAB deep learning toolbox etc. are not allowed). [Marks=5]\n",
        "import scipy.io\n",
        "path = \"/content/drive/My Drive/Neural_Nets_Assignment_2/data5.mat\"\n",
        "mat = scipy.io.loadmat(path)\n",
        "np.random.seed(0)\n",
        "\n",
        "data = mat['x']\n",
        "np.random.shuffle(data)       \n",
        "\n",
        "test_size = int(0.2*data.shape[0])                  \n",
        "val_size = int(0.1*data.shape[0])\n",
        "\n",
        "test_data = data[:test_size]                       \n",
        "val_data = data[test_size:(test_size+val_size)]\n",
        "train_data = data[(test_size+val_size):]\n",
        "\n",
        "train_data5_x = train_data[:,:-1]\n",
        "Y_train = train_data[:,-1]\n",
        "X_test = test_data[:,:-1]\n",
        "Y_test = test_data[:,-1]\n",
        "\n",
        "Y_train = np.reshape(Y_train, (-1, 1))\n",
        "Y_test = np.reshape(Y_test, (-1, 1))\n",
        "\n",
        "n = train_data5_x.shape[1]\n",
        "train_mean = np.zeros(n)\n",
        "train_std = np.zeros(n)\n",
        "\n",
        "for j in range(n): \n",
        "  train_mean[j] = train_data5_x[:,j].mean()\n",
        "  train_std[j] = train_data5_x[:,j].std()\n",
        "  train_data5_x[:,j] = (train_data5_x[:,j]-train_data5_x[:,j].mean())/(train_data5_x[:,j].std())\n",
        "\n",
        "for j in range(n):                                      \n",
        "  X_test[:,j] = (X_test[:,j]-train_mean[j])/(train_std[j]) \n",
        "\n",
        "\n",
        "model = multilayer_Perceptron(train_data5_x, np.array(Y_train))\n",
        "\n",
        "epochs = 3000\n",
        "for x in range(epochs):\n",
        "    model.feedforward()\n",
        "    model.backprop()\n",
        "\t\t\n",
        "def get_acc(x, y):\n",
        "    acc = 0\n",
        "    for xx,yy in zip(x, y):\n",
        "        s = model.predict(xx)\n",
        "        if s == yy:\n",
        "            acc +=1\n",
        "    return acc/len(x)*100\n",
        "\t\n",
        "print(\"Training accuracy : \", get_acc(train_data5_x, np.array(Y_train)))\n",
        "print(\"Test accuracy : \", get_acc(X_test, np.array(Y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy :  99.13621262458472\n",
            "Test accuracy :  92.07459207459208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SBKqi4pGafi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "outputId": "106698cf-bc1b-4360-985f-c3f1dc04016b"
      },
      "source": [
        "#@title Implement kernel perceptron algorithm for the classification. The dataset (data55.mat) contains 4 features and the last column is the output (class label). You can use hold-out cross-validation (70, 10, and 20%) for the selection of training, validation and test instances. Evaluate accuracy, sensitivity and specificity measures for the evaluation of test instances. (You can use RBF, and polynomial kernels). (Packages such as keras, tensorflow, pytorch for python and MATLAB deep learning toolbox etc. are not allowed)\n",
        "# TODO: Implement a user input choice for the kernels\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Training\n",
        "    alpha = [0] * len(train_data55) # mistake counter\n",
        "    iterations = 2 # Converges in 2 iterations\n",
        "    for i in range(iterations):\n",
        "        for j in range(len(train_data55)):\n",
        "            lin_comb = 0\n",
        "            for k in range(len(train_data55)):\n",
        "                if (int(train_data55.iat[k, 4]) == 0):\n",
        "                    # lin_comb += alpha[k] * -1 * polynomial_kernel_function(train_data55.iloc[k][0:-1], train_data55.iloc[j][0:-1], 4)\n",
        "                    lin_comb += alpha[k] * -1 * rbf_kernel_function(train_data55.iloc[k][0:-1], train_data55.iloc[j][0:-1], 4)\n",
        "                else:\n",
        "                    # lin_comb += alpha[k] * 1 * polynomial_kernel_function(train_data55.iloc[k][0:-1], train_data55.iloc[j][0:-1], 4)\n",
        "                    lin_comb += alpha[k] * 1 * rbf_kernel_function(train_data55.iloc[k][0:-1], train_data55.iloc[j][0:-1], 4)\n",
        "            y_hat = np.sign(lin_comb)\n",
        "            # TODO: Refactor below\n",
        "            if (y_hat == -1):\n",
        "                y_hat = 0 # As truth label in given data is {0, 1}\n",
        "            else:\n",
        "                y_hat = 1\n",
        "            if (y_hat != int(train_data55.iat[j, 4])):\n",
        "                alpha[j] += 1\n",
        "\n",
        "    # Validation\n",
        "    error = 0\n",
        "    for i in range(len(validate_data55[0])):\n",
        "        lin_comb = 0\n",
        "        for j in range(len(train_data55)):\n",
        "            if (int(train_data55.iat[j, 4]) == 0):\n",
        "                # lin_comb += alpha[j] * -1 * polynomial_kernel_function(train_data55.iloc[j][0:-1], test_data55.iloc[i][0:-1], 4)\n",
        "                lin_comb += alpha[j] * -1 * rbf_kernel_function(train_data55.iloc[j][0:-1], test_data55.iloc[i][0:-1], 4)\n",
        "            else:\n",
        "                # lin_comb += alpha[j] * 1 * polynomial_kernel_function(train_data55.iloc[j][0:-1], test_data55.iloc[i][0:-1], 4)\n",
        "                lin_comb += alpha[j] * 1 * rbf_kernel_function(train_data55.iloc[j][0:-1], test_data55.iloc[i][0:-1], 4)\n",
        "        y_test = np.sign(lin_comb)\n",
        "        if (int(validate_data55.iat[i, 4]) != y_test):\n",
        "            error += 1\n",
        "    print(\"Error in validation set is %d\" % (error / len(validate_data55[0])))\n",
        "\n",
        "    # Testing\n",
        "    confusion_matrix = [[0, 0], [0, 0]]\n",
        "    for i in range(len(test_data55[0])):\n",
        "        lin_comb = 0\n",
        "        for j in range(len(train_data55)):\n",
        "            if (int(train_data55.iat[j, 4]) == 0):\n",
        "                # lin_comb += alpha[j] * -1 * polynomial_kernel_function(train_data55.iloc[j][0:-1], test_data55.iloc[i][0:-1], 4)\n",
        "                lin_comb += alpha[j] * -1 * rbf_kernel_function(train_data55.iloc[j][0:-1], test_data55.iloc[i][0:-1], 4)\n",
        "            else:\n",
        "                # lin_comb += alpha[j] * 1 * polynomial_kernel_function(train_data55.iloc[j][0:-1], test_data55.iloc[i][0:-1], 4)\n",
        "                lin_comb += alpha[j] * 1 * rbf_kernel_function(train_data55.iloc[j][0:-1], test_data55.iloc[i][0:-1], 4)\n",
        "        y_test = np.sign(lin_comb)\n",
        "        # TODO: Refactor below\n",
        "        if (y_test == -1):\n",
        "            y_test = 0\n",
        "        else:\n",
        "            y_test = 1\n",
        "        confusion_matrix[int(test_data55.iat[i, 4])][y_test] += 1\n",
        "    print(\"Confusion matrix is %s\" % confusion_matrix)\n",
        "    stat_measures(confusion_matrix)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error in validation set is 0\n",
            "Confusion matrix is [[9, 0], [0, 11]]\n",
            "Accuracy is 1\n",
            "Specificity is 1\n",
            "Sensitivity is 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSgTVvUQ-RFP",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0c0ab1-941b-44e3-e5df-dff5502593ca"
      },
      "source": [
        "#@title Implement the radial basis function neural network (RBFNN) for the classification problem. You can use Gaussian, multiquadric and linear kernel functions for the implementation. You can use both holdout (70, 10, and 20%) and 5-fold cross-validation approaches for evaluating the performance of the classifier (individual accuracy and overall accuracy). The dataset (data5.mat) contains 72 features and the last column is the output (class labels). (Packages such as keras, tensorflow, pytorch for python and MATLAB deep learning toolbox etc. are not allowed). [Marks=5]\n",
        "def gaussian(x1, sigma=1):\n",
        "  return np.exp(- (x1) ** 2 / (2 * sigma ** 2))\n",
        "if __name__ == \"__main__\":\n",
        "    # Parameters\n",
        "    K = 100 # Number of clusters\n",
        "\n",
        "    # Split the data into input and labels and one hot encode class labels\n",
        "    train_data5_x = train_data5[0:, 0:-1]\n",
        "    train_data5_y = create_one_hot_encoding(train_data5[0:, -1], (len(train_data5[0:, -1]), 2))\n",
        "    validate_data5_x = validate_data5[0:, 0:-1]\n",
        "    validate_data5_y = create_one_hot_encoding(validate_data5[0:, -1], (len(validate_data5[0:, -1]), 2))\n",
        "    test_data5_x = test_data5[0:, 0:-1]\n",
        "    test_data5_y = create_one_hot_encoding(test_data5[0:, -1], (len(test_data5[0:, -1]), 2))\n",
        "    \n",
        "    centroids = kmeans(train_data5_x, K)\n",
        "    hidden_matrix = np.zeros((len(train_data5), len(centroids)))\n",
        "\n",
        "    # Training\n",
        "    print(\"Linear Kernel\")\n",
        "    for i in range(len(train_data5)):\n",
        "        for j in range(len(centroids)):\n",
        "            hidden_matrix[i][j] = (np.linalg.norm(train_data5_x[i] - centroids[j])) # Linear Kernel\n",
        "    \n",
        "    hidden_matrix = np.linalg.pinv(hidden_matrix)\n",
        "    weight_matrix = hidden_matrix @ train_data5_y\n",
        "\n",
        "    # Testing\n",
        "    hidden_test_matrix = np.zeros((len(test_data5), len(centroids)))\n",
        "    for i in range(len(test_data5)):\n",
        "        for j in range(len(centroids)):\n",
        "            hidden_test_matrix[i][j] = (np.linalg.norm(test_data5_x[i] - centroids[j])) # Linear Kernel\n",
        "    \n",
        "    y_pred = hidden_test_matrix @ weight_matrix\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    test_data5_y = np.argmax(test_data5_y, axis=1)\n",
        "    print(pd.crosstab(y_pred, test_data5_y))\n",
        "    print(np.sum(y_pred==test_data5_y)/len(test_data5_y))\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Kernel\n",
            "col_0    0    1\n",
            "row_0          \n",
            "0      178   14\n",
            "1       38  200\n",
            "0.8790697674418605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNM-_WuZqy4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "outputId": "99ab6649-7f37-4491-c402-f302d43bc934"
      },
      "source": [
        "#@title Implement the stacked autoencoder based deep neural network for the classification problem. The deep neural network must contain 3 hidden layers from three autoencoders. You can use holdout (70, 10, and 20%) cross-validation technique for selecting, training and test instances for the classifier. The dataset (data5.mat) contains 72 features and the last column is the output (class labels). For autoencoder implementation, please use back propagation algorithm which has been already taught in the class. Evaluate individual accuracy and overall accuracy. (Packages such as keras, tensorflow, pytorch for python and MATLAB deep learning toolbox etc. are not allowed) [Marks=5]\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "test = sio.loadmat('/content/drive/My Drive/Neural_Nets_Assignment_2/data5.mat') #importing and loading the data\n",
        "df=test['x']\n",
        "\n",
        "def layer(number_of_neurons, number_of_features):\n",
        "    return 2 * np.random.random((number_of_features + 1, number_of_neurons)) - 1\n",
        "\n",
        "def weights_init(X_train, y_train, X_test, y_test, learning_rate, hiddenlayer1, hiddenlayer2,hiddenlayer3):\n",
        "    w1 = layer(hiddenlayer1, X_train.shape[1])\n",
        "    w2 = layer(hiddenlayer2, hiddenlayer1)\n",
        "    w3 = layer(hiddenlayer3, hiddenlayer2)\n",
        "    output_weights = layer(1, hiddenlayer3)\n",
        "    return w1, w2, w3, output_weights\n",
        "\n",
        "def train(X_train, y_train, X_test, y_test, learning_rate, hiddenlayer1, hiddenlayer2, hiddenlayer3):\n",
        "\n",
        "    w1, w2, w3, output_weights = weights_init(X_train, y_train, X_test, y_test, learning_rate, hiddenlayer1, hiddenlayer2, hiddenlayer3)\n",
        "    alpha = 0.01\n",
        "    iterations = 30\n",
        "    for iterations in range(iterations):\n",
        "        for i in range(X_train.shape[0]):\n",
        "            X_vector = X_train[i]\n",
        "            y_actual = y_train[i]\n",
        "            hidden_layer1 = forward_propagation(w1, X_vector)\n",
        "            hidden_layer2 = forward_propagation(w2, hidden_layer1)\n",
        "            hidden_layer3 = forward_propagation(w3, hidden_layer2)\n",
        "            class_label = forward_propagation(output_weights, hidden_layer3)\n",
        "            output_weights = backpropogation4(output_weights, hidden_layer3, y_actual, class_label, learning_rate)\n",
        "\n",
        "            w3 = backpropogation3(w3, hidden_layer2, output_weights, hidden_layer3, y_actual, class_label, learning_rate)\n",
        "\n",
        "            w2 = backpropogation2(w2, hidden_layer1, w3, hidden_layer2, output_weights,hidden_layer3, y_actual, class_label, learning_rate)\n",
        "\n",
        "            w1 = backpropogation1(w1, X_vector, w2, hidden_layer1, w3, hidden_layer2,output_weights, hidden_layer3, y_actual, class_label, learning_rate)\n",
        "    y_pred = []\n",
        "\n",
        "    for j in range(X_test.shape[0]):\n",
        "        X_vector = X_test[j]\n",
        "        hidden_layer1 = forward_propagation(w1, X_vector)\n",
        "        hidden_layer2 = forward_propagation(w2, hidden_layer1)\n",
        "        hidden_layer3 = forward_propagation(w3, hidden_layer2)\n",
        "        prediction = forward_propagation(output_weights, hidden_layer3)\n",
        "        y_pred.append(prediction)\n",
        "        \n",
        "    y_pred = np.squeeze(np.array(y_pred))\n",
        "    y_class_prediction = np.where(y_pred>0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_class_prediction, y_test)\n",
        "    \n",
        "    print(\"ACCURACY IS \", accuracy)\n",
        "    \n",
        "    print(\"CONFUSION MATRIX IS \")\n",
        "    print(confusion_matrix(y_class_prediction, y_test))\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    mat = sio.loadmat('/content/drive/My Drive/Neural_Nets_Assignment_2/data5.mat')\n",
        "    dataset = pd.DataFrame(mat['x']).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    #Parameters\n",
        "    learning_rate = 0.01\n",
        "    # Number of neurons\n",
        "    hiddenlayer1 = 400\n",
        "    hiddenlayer2 = 200\n",
        "    hiddenlayer3 = 400\n",
        "\n",
        "    # Split and normalize\n",
        "    train_data_5_x=dataset.iloc[:,:72].values\n",
        "    X_normalized = (train_data_5_x-train_data_5_x.mean(0))/train_data_5_x[:,0].std(0)\n",
        "    y=dataset.iloc[:,72].values.astype(int)\n",
        "    \n",
        "    split=int(np.round(0.6*X_normalized.shape[0]))\n",
        "    X_train=X_normalized[:split,:]\n",
        "    X_test=X_normalized[split:]\n",
        "    \n",
        "    y_train=y[:split]\n",
        "    y_test=y[split:]\n",
        "\n",
        "    train(X_train, y_train, X_test, y_test, learning_rate, hiddenlayer1, hiddenlayer2, hiddenlayer3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACCURACY IS  0.930151338766007\n",
            "CONFUSION MATRIX IS \n",
            "[[375  24]\n",
            " [ 36 424]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "id": "3D7qGtATgUYz",
        "outputId": "c07bdb25-aecd-4388-a942-8796e33dbd61"
      },
      "source": [
        "#@title Implement extreme learning machine (ELM) classifier for the classification. You can use Gaussian and tanh activation functions. Please select the training and test instances using 5-fold cross-validation technique Evaluate individual accuracy and overall accuracy. The dataset (data5.mat) contains 72 features and the last column is the output (class labels). (Packages such as keras, tensorflow, pytorch for python and MATLAB deep learning toolbox etc. are not allowed) [Marks=5]\n",
        "\n",
        "def kfold(z,i):\n",
        "    data=z.copy()\n",
        "    test=z[i]\n",
        "    del data[i]\n",
        "    train=np.concatenate(data)\n",
        "    return train,test\n",
        "\n",
        "def tanh(X):\n",
        "  return np.tanh(X)\n",
        "\n",
        "def gaussian(x1, sigma=1):\n",
        "  return np.exp(- (x1) ** 2 / (2 * sigma ** 2))\n",
        "\n",
        "# Parameters\n",
        "n_hidden_neurons = 300\n",
        "\n",
        "train_data5_x = train_data5[0:, 0:-1]\n",
        "train_data5_y = create_one_hot_encoding(train_data5[0:, -1], (len(train_data5[0:, -1]), 2))\n",
        "validate_data5_x = validate_data5[0:, 0:-1]\n",
        "validate_data5_y = create_one_hot_encoding(validate_data5[0:, -1], (len(validate_data5[0:, -1]), 2))\n",
        "test_data5_x = test_data5[0:, 0:-1]\n",
        "test_data5_y = create_one_hot_encoding(test_data5[0:, -1], (len(test_data5[0:, -1]), 2))\n",
        "\n",
        "split = np.array_split(data5, 5)\n",
        "\n",
        "print(\"FOR tanh activation\")\n",
        "for i in range(5):\n",
        "    print(\"Fold \", i + 1)\n",
        "    train, test = kfold(split, i)\n",
        "    train_data5_x = train[:,:72]\n",
        "    train_data5_y = create_one_hot_encoding(train[:, 72], (len(train[:, 72]), 2))\n",
        "    test_data5_x = test[:,:72]\n",
        "    test_data5_y = create_one_hot_encoding(test[:, 72], (len(test[:, 72]), 2))\n",
        "\n",
        "    randommat = np.random.randn(train_data5_x.shape[1] + 1, n_hidden_neurons)\n",
        "    tempH = np.append(np.ones((train_data5_x.shape[0], 1)), train_data5_x, axis=1) @ randommat;\n",
        "    H = tanh(tempH)\n",
        "    OutputWeight = np.linalg.pinv(H) @ train_data5_y\n",
        "\n",
        "    testH = np.append(np.ones((test_data5_x.shape[0], 1)), test_data5_x, axis=1) @ randommat;\n",
        "    Ht = tanh(testH)\n",
        "    y_pred = Ht @ OutputWeight\n",
        "\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    test_data5_y = np.argmax(test_data5_y, axis=1)\n",
        "    print(pd.crosstab(y_pred, test_data5_y))\n",
        "    print(np.sum(y_pred==test_data5_y)/len(test_data5_y))\n",
        "\n",
        "print(\"FOR gaussian activation\")\n",
        "for i in range(5):\n",
        "    print(\"Fold \", i + 1)\n",
        "    train, test = kfold(split, i)\n",
        "    train_data5_x = train[:,:72]\n",
        "    train_data5_y = create_one_hot_encoding(train[:, 72], (len(train[:, 72]), 2))\n",
        "    test_data5_x = test[:,:72]\n",
        "    test_data5_y = create_one_hot_encoding(test[:, 72], (len(test[:, 72]), 2))\n",
        "\n",
        "    randommat = np.random.randn(train_data5_x.shape[1] + 1, n_hidden_neurons)\n",
        "    tempH = np.append(np.ones((train_data5_x.shape[0], 1)), train_data5_x, axis=1) @ randommat\n",
        "    H = gaussian(tempH)\n",
        "    OutputWeight = np.linalg.pinv(H) @ train_data5_y\n",
        "\n",
        "    testH = np.append(np.ones((test_data5_x.shape[0], 1)), test_data5_x, axis=1) @ randommat\n",
        "    Ht = gaussian(testH)\n",
        "    y_pred = Ht @ OutputWeight\n",
        "\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    test_data5_y = np.argmax(test_data5_y, axis=1)\n",
        "    print(pd.crosstab(y_pred, test_data5_y))\n",
        "    print(np.sum(y_pred==test_data5_y)/len(test_data5_y))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOR tanh activation\n",
            "Fold  1\n",
            "col_0    0    1\n",
            "row_0          \n",
            "0      170   23\n",
            "1       44  193\n",
            "0.8441860465116279\n",
            "Fold  2\n",
            "col_0    0    1\n",
            "row_0          \n",
            "0      185   39\n",
            "1       27  179\n",
            "0.8465116279069768\n",
            "Fold  3\n",
            "col_0    0    1\n",
            "row_0          \n",
            "0      184   31\n",
            "1       29  186\n",
            "0.8604651162790697\n",
            "Fold  4\n",
            "col_0    0    1\n",
            "row_0          \n",
            "0      185   23\n",
            "1       30  191\n",
            "0.8764568764568764\n",
            "Fold  5\n",
            "col_0    0    1\n",
            "row_0          \n",
            "0      183   34\n",
            "1       38  174\n",
            "0.8321678321678322\n",
            "FOR gaussian activation\n",
            "Fold  1\n",
            "col_0    0    1\n",
            "row_0          \n",
            "0      214  213\n",
            "1        0    3\n",
            "0.5046511627906977\n",
            "Fold  2\n",
            "col_0    0    1\n",
            "row_0          \n",
            "0      212  218\n",
            "0.4930232558139535\n",
            "Fold  3\n",
            "col_0    0    1\n",
            "row_0          \n",
            "0      211  217\n",
            "1        2    0\n",
            "0.4906976744186046\n",
            "Fold  4\n",
            "col_0    0    1\n",
            "row_0          \n",
            "0      215  212\n",
            "1        0    2\n",
            "0.5058275058275058\n",
            "Fold  5\n",
            "col_0    0    1\n",
            "row_0          \n",
            "0      219  208\n",
            "1        2    0\n",
            "0.5104895104895105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzrD2Deu7HLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "189e5038-a1a2-4038-f27a-dfd19b16b019"
      },
      "source": [
        "#@title Implement support vector machine (SVM) classifier for the multi-class classification task. You can use one vs one and one vs all multiclass coding methods to create binary SVM models. Implement the SMO algorithm for the evaluation of the training parameters of SVM such as Lagrange multipliers. You can use holdout approach (70, 10, 20%) for evaluating the performance of the classifier. The dataset (data5.mat) contains 72 features and the last column is the output (class labels). Evaluate individual accuracy and overall accuracy. (Packages such as keras, tensorflow, pytorch for python and MATLAB deep learning toolbox etc. are not allowed) [Marks=10]\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "train_data5_x = train_data5[0:, 0:-1]\n",
        "train_data5_y = train_data5[0:, -1]\n",
        "test_data5_x = test_data5[0:, 0:-1]\n",
        "test_data5_y = test_data5[0:, -1]\n",
        "validate_data5_x = validate_data5[0:, 0:-1]\n",
        "validate_data5_y = validate_data5[0:, -1]\n",
        "\n",
        "train_data5_x = normalize(train_data5_x)\n",
        "test_data5_x = normalize(test_data5_x)\n",
        "train_data5_y = np.where(train_data5_y == 0, -1, 1)\n",
        "test_data5_y = np.where(test_data5_y == 0, -1, 1)\n",
        "\n",
        "def update_bias(X, y, w):\n",
        "  b_tmp = y - np.dot(w.T, X.T)\n",
        "  return np.mean(b_tmp)\n",
        "\n",
        "def update_weight(alpha, y, X):\n",
        "  return np.dot(X.T, np.multiply(alpha, y))\n",
        "\n",
        "def error(x_k, y_k, w, b):\n",
        "  return h(x_k, w, b) - y_k\n",
        "\n",
        "def randint(a, b, z):\n",
        "  i = z\n",
        "  while i == z:\n",
        "        i = rnd.randint(a,b)\n",
        "  return i\n",
        "\n",
        "def model(X, y,max_iter=100, C=1.0, epsilon=0.0001):\n",
        "        n, d = X.shape[0], X.shape[1]\n",
        "        alpha = np.zeros((n))\n",
        "        kernel = gaussian_function # Using gaussian kernel\n",
        "        count = 0\n",
        "        while True:\n",
        "            alpha_prev = np.copy(alpha)\n",
        "            for j in range(0, n):\n",
        "                i = randint(0, n-1, j)\n",
        "                x_i, x_j, y_i, y_j = X[i,:], X[j,:], y[i], y[j]\n",
        "                k_ij = kernel(x_i, x_i) + kernel(x_j, x_j) - 2 * kernel(x_i, x_j)\n",
        "                if k_ij == 0:\n",
        "                    continue\n",
        "                alpha_prime_j, alpha_prime_i = alpha[j], alpha[i]\n",
        "                (L, H) = l_h_comp(C, alpha_prime_j, alpha_prime_i, y_j, y_i)\n",
        "\n",
        "                # Weights and bias\n",
        "                weight = update_weight(alpha, y, X)\n",
        "                bias = update_bias(X, y, weight)\n",
        "\n",
        "                # Errors\n",
        "                E_i = error(x_i, y_i, weight, bias)\n",
        "                E_j = error(x_j, y_j, weight, bias)\n",
        "\n",
        "                # Alpha values\n",
        "                alpha[j] = alpha_prime_j + float(y_j * (E_i - E_j))/k_ij\n",
        "                alpha[j] = max(alpha[j], L)\n",
        "                alpha[j] = min(alpha[j], H)\n",
        "                alpha[i] = alpha_prime_i + y_i*y_j * (alpha_prime_j - alpha[j])\n",
        "\n",
        "            # Convergence\n",
        "            diff = np.linalg.norm(alpha - alpha_prev)\n",
        "            if diff < epsilon:\n",
        "                break\n",
        "                \n",
        "        bias = update_bias(X, y, weight)\n",
        "\n",
        "        # Support vectors\n",
        "        alpha_idx = np.where(alpha > 0)[0]\n",
        "        support_vectors = X[alpha_idx, :]\n",
        "        return weight, bias\n",
        "\n",
        "weight, bias = model(train_data5_x,train_data5_y)\n",
        "y_predicted = predict(test_data5_x,weight,bias)\n",
        "\n",
        "conf_mat = confusion_matrix(test_data5_y, y_predicted)\n",
        "print(\"Confusion matrix is :\")\n",
        "print(conf_mat)\n",
        "accuracy = (conf_mat[0][0] + conf_mat[1][1]) / (conf_mat[0][0] + conf_mat[0][1] + conf_mat[1][0] + conf_mat[1][1])\n",
        "print(\"Accuracy is :\")\n",
        "print(accuracy)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix is :\n",
            "[[169  41]\n",
            " [ 31 189]]\n",
            "Accuracy is :\n",
            "0.8325581395348837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGslLGPVsBQ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3aad0daf-b73f-43c0-d7e5-d84da79d497a"
      },
      "source": [
        "#@title Implement a multi-channel 1D deep CNN architecture for the seven-class classification task. The input and the class labels are given in .mat file format. There is a total of 17160 number of instances present in both input and class-label data files. The input data for each instance is a multichannel time series (12-channel) with size as (12 ×800). The class label for each multichannel time series instance is given in the class_label.mat file. You can select the training and test instances using hold- out cross-validation (70% training, 10% validation, and 20% testing). The architecture of the multi-channel deep CNN is given as follows. The number of filters, length of each filter, and number of neurons in the fully connected layers are shown in the following figure. Evaluate individual accuracy and overall accuracy. (Packages such as keras, tensorflow, pytorch for python and MATLAB deep learning toolbox etc. are allowed)[Marks=10]\n",
        "# Training the model\n",
        "def build_model(train):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(kernel_size=7, filters=20,activation='relu', input_shape=(800,12)))\n",
        "    model.add(MaxPooling1D(pool_size=3, strides=3))\n",
        "    model.add(Conv1D(kernel_size=7,filters=60,activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=3,strides=3))\n",
        "    model.add(Dropout(0.7))\n",
        "    model.add(Conv1D(filters=120,kernel_size=7))\n",
        "    model.add(Conv1D(filters=120,kernel_size=7))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(2000,activation='relu'))\n",
        "    model.add(Dense(700,activation='relu'))\n",
        "    model.add(Dense(50,activation='relu'))\n",
        "    model.add(Dense(7,activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    history = model.fit(train_input, train_class_label, epochs=5, batch_size=1000,validation_data=(validate_input, validate_class_label))\n",
        "    print(history)\n",
        "    return history, model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    history, model = build_model(train_input)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "\n",
        "    y_pred = np.argmax(model.predict(test_input))\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_4 (Conv1D)            (None, 794, 20)           1700      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 264, 20)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 258, 60)           8460      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 86, 60)            0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 86, 60)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 80, 120)           50520     \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 74, 120)           100920    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8880)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2000)              17762000  \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 700)               1400700   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 50)                35050     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 7)                 357       \n",
            "=================================================================\n",
            "Total params: 19,359,707\n",
            "Trainable params: 19,359,707\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "13/13 [==============================] - 3s 240ms/step - loss: 1.1861 - accuracy: 0.4810 - val_loss: 2.2023 - val_accuracy: 0.5851\n",
            "Epoch 2/5\n",
            "13/13 [==============================] - 2s 175ms/step - loss: 0.6472 - accuracy: 0.7962 - val_loss: 0.1513 - val_accuracy: 0.8986\n",
            "Epoch 3/5\n",
            "13/13 [==============================] - 2s 173ms/step - loss: 0.1228 - accuracy: 0.9177 - val_loss: 0.0531 - val_accuracy: 0.9411\n",
            "Epoch 4/5\n",
            "13/13 [==============================] - 2s 175ms/step - loss: 0.0544 - accuracy: 0.9472 - val_loss: 0.0473 - val_accuracy: 0.9645\n",
            "Epoch 5/5\n",
            "13/13 [==============================] - 2s 172ms/step - loss: 0.0404 - accuracy: 0.9694 - val_loss: 0.0169 - val_accuracy: 0.9843\n",
            "<tensorflow.python.keras.callbacks.History object at 0x7fc9e01fde10>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_YfzUe0QamZ"
      },
      "source": [
        "test_class_label=test_class_label+1\n",
        "y_pred=np.argmax(model.predict(test_input), axis=-1)+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFQO6Zj7TJV_",
        "outputId": "78ab12a7-422c-4d9d-ab4a-1c20d413b344"
      },
      "source": [
        "test_class_label = test_class_label.T.flatten()\n",
        "\n",
        "cm=confusion_matrix(test_class_label, y_pred)\n",
        "print(\"Confusion matrix is\")\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix is\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[549,   0,  37,   0,   0,   0,   0],\n",
              "       [  0, 388,   0,   0,   0,   0,   0],\n",
              "       [  0,   0, 555,   0,   0,   0,   0],\n",
              "       [  0,   0,   0, 307,   0,   0,   0],\n",
              "       [  0,   0,   0,   0, 601,   0,   0],\n",
              "       [  0,   0,   0,   0,   0, 621,   0],\n",
              "       [  0,   0,   0,   0,   0,   0, 374]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VezVh4OVRDM",
        "outputId": "602fbecf-1dd3-4f5e-9816-3c223c6ab78f"
      },
      "source": [
        "print(\"Individual class acc\")\n",
        "cm=confusion_matrix(test_class_label,y_pred)\n",
        "for i in range(7):\n",
        "  print(\"class \", i+1)\n",
        "  print((cm.diagonal()/cm.sum(axis=1))[i] *100) #Individual Class Accuracy\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Individual class acc\n",
            "class  1\n",
            "93.68600682593856\n",
            "class  2\n",
            "100.0\n",
            "class  3\n",
            "100.0\n",
            "class  4\n",
            "100.0\n",
            "class  5\n",
            "100.0\n",
            "class  6\n",
            "100.0\n",
            "class  7\n",
            "100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4R3Z0g6l5AL"
      },
      "source": [
        "import os \n",
        "import glob \n",
        "import PIL\n",
        "import keras \n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as sio \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings \n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Input, Dense, Activation, Flatten, Conv2D, Dropout,MaxPooling2D,MaxPool2D,BatchNormalization\n",
        "from keras.optimizers import SGD,Adam\n",
        "from keras.utils import np_utils\n",
        "from sklearn.utils import shuffle\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zafmvq7__OFq",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "5b62c925-dd7e-4698-ffd8-59bff2111dfd"
      },
      "source": [
        "#@title The two-dimensional time-frequency images of class1, class2, class3 are given in the folders as class1.zip, class2.zip, class3.zip, respectively. Design a 2D deep CNN classifier for the three-class classification. Evaluate the classification performance using hold-out cross-validation (70% training, 10% validation, 20% testing), and 10-fold cross-validation methods. Evaluate individual accuracy and overall accuracy for the multiclass CNN classifier. You can consider 4 convolutional layer, three pooling layer, and 5 fully connected layers. You can select the number of filters, stride for convolution and pooling layers, and number of neurons for fully connected layers as per your own choice. (Packages such as keras, tensorflow, pytorch for python and MATLAB deep learning toolbox etc. are allowed). You can apply dropout, batch normalization, and regularization to improve the classification performance. [Marks=10]\n",
        "def model_10():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters = 8, kernel_size = (3,3), input_shape=(128, 128, 3), padding = \"same\"))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  \n",
        "  model.add(Conv2D(filters = 8, kernel_size = (3,3), padding = \"same\"))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'))\n",
        "  \n",
        "  model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = \"same\"))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  \n",
        "  model.add(Conv2D(filters = 32, kernel_size = (3,3), padding =\"same\"))\n",
        "  model.add(Flatten())\n",
        "  \n",
        "  model.add(Dense(128))\n",
        "  model.add(Activation('relu'))\n",
        "  \n",
        "  model.add(Dense(64))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Activation('relu'))\n",
        "  \n",
        "  model.add(Dense(32))\n",
        "  model.add(Activation('relu'))\n",
        "  \n",
        "  model.add(Dense(16))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Activation('relu'))\n",
        "  \n",
        "  model.add(Dense(3))\n",
        "  model.add(Activation('softmax'))\n",
        "  return model\n",
        "\n",
        "\n",
        "images_x = []\n",
        "y = []\n",
        "path_of_image = \"/content/drive/MyDrive/class1_images.zip (Unzipped Files)/class1_images\"  \n",
        "data_path = os.path.join(path_of_image,'*g') \n",
        "files = glob.glob(data_path) \n",
        "\n",
        "for f1 in files:\n",
        "  image_array = np.asarray((Image.open(f1)\n",
        "  images_x.append(image_array).resize((128,128))))\n",
        "  y.append(0)\n",
        "path_of_image = \"/content/drive/MyDrive/class2_images.zip (Unzipped Files)/class2_images\"  \n",
        "data_path = os.path.join(path_of_image,'*g') \n",
        "files = glob.glob(data_path) \n",
        "\n",
        "for f2 in files:\n",
        "  image_array = np.asarray((Image.open(f2)\n",
        "  images_x.append().resize((128,128))))\n",
        "  y.append(1)\n",
        "path_of_image = \"/content/drive/MyDrive/class3_images.zip (Unzipped Files)/class3_images\"  \n",
        "data_path = os.path.join(path_of_image,'*g') \n",
        "files = glob.glob(data_path) \n",
        "\n",
        "for f3 in files:\n",
        "  image_array = np.asarray((Image.open(f3)))\n",
        "  images_x.append().resize((128,128))))\n",
        "  y.append(2)\n",
        "images_x = np.asarray(images_x)\n",
        "y = np.asarray(y)\n",
        "images_x = images_x/255.0\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "train_images_x, test_images_x, train_y, test_y = train_test_split(images_x, y, test_size = 2/10, train_size = 8/10,random_state = 0)\n",
        "train_images_x, valid_images_x, train_y, valid_y = train_test_split(train_images_x, train_y, test_size = 1/8, train_size = 7/8, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-75aad292df5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0mimage_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m   \u001b[0mimages_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m   \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mpath_of_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/class2_images.zip (Unzipped Files)/class2_images\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'resize'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbGpL9GXlxyr",
        "outputId": "e7ed2d3d-b19c-4c08-839f-882a91066a7b"
      },
      "source": [
        "model_10_instance=model_10()\n",
        "model_10_instance.summary()\n",
        "model_10_instance.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history=model_10_instance.fit(train_images_x, train_y, epochs=20, batch_size=15,validation_data=(valid_images_x,valid_y),shuffle=False)\n",
        "print(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 128, 128, 8)       224       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 128, 128, 8)       0         \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 128, 128, 8)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 128, 128, 8)       584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 128, 128, 8)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 128, 128, 32)      2336      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 128, 128, 32)      9248      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 524288)            0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 128)               67108992  \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 67,132,299\n",
            "Trainable params: 67,132,299\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 504 samples, validate on 72 samples\n",
            "Epoch 1/20\n",
            "504/504 [==============================] - 2s 4ms/sample - loss: 1.9062 - accuracy: 0.2917 - val_loss: 1.0975 - val_accuracy: 0.3194\n",
            "Epoch 2/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 1.1199 - accuracy: 0.3651 - val_loss: 1.0939 - val_accuracy: 0.3056\n",
            "Epoch 3/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 1.0975 - accuracy: 0.3869 - val_loss: 1.0899 - val_accuracy: 0.3056\n",
            "Epoch 4/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 1.0553 - accuracy: 0.4325 - val_loss: 0.9466 - val_accuracy: 0.5556\n",
            "Epoch 5/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.8552 - accuracy: 0.6151 - val_loss: 0.4940 - val_accuracy: 0.9861\n",
            "Epoch 6/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.3514 - accuracy: 0.8611 - val_loss: 0.0310 - val_accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0881 - accuracy: 0.9643 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0365 - accuracy: 0.9841 - val_loss: 2.9994e-04 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0274 - accuracy: 0.9881 - val_loss: 5.1179e-04 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0243 - accuracy: 0.9861 - val_loss: 2.7095e-05 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0148 - accuracy: 0.9940 - val_loss: 9.0731e-07 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0252 - accuracy: 0.9921 - val_loss: 1.1194e-05 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0158 - accuracy: 0.9901 - val_loss: 0.0220 - val_accuracy: 0.9861\n",
            "Epoch 14/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0104 - accuracy: 0.9960 - val_loss: 6.9786e-05 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0072 - accuracy: 0.9960 - val_loss: 1.0377e-04 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0119 - accuracy: 0.9921 - val_loss: 5.1881e-06 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0115 - accuracy: 0.9960 - val_loss: 1.3871e-05 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0071 - accuracy: 0.9960 - val_loss: 0.0868 - val_accuracy: 0.9861\n",
            "Epoch 19/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0054 - accuracy: 0.9980 - val_loss: 0.1225 - val_accuracy: 0.9861\n",
            "Epoch 20/20\n",
            "504/504 [==============================] - 2s 3ms/sample - loss: 0.0065 - accuracy: 0.9960 - val_loss: 0.0900 - val_accuracy: 0.9861\n",
            "<tensorflow.python.keras.callbacks.History object at 0x7fdb50107780>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-W9JoUX73mX",
        "outputId": "29aea9be-71a9-40a6-feb1-00479503f547"
      },
      "source": [
        "test_y = test_y+1\n",
        "y_pred = np.argmax(model_10_instance.predict(test_images_x), axis=-1)+1\n",
        "cm = pd.crosstab(test_y,y_pred)\n",
        "print(\"Confusion matrix is\")\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix is\n",
            "col_0   1   2   3\n",
            "row_0            \n",
            "1      42   0   0\n",
            "2       0  54   0\n",
            "3       0   0  48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEU-VyCoIEsD",
        "outputId": "2a20ebde-1ded-48bb-8bb0-e8442ed952f9"
      },
      "source": [
        "print(\"cross validation \")\n",
        "print(\"Individual class accuracy:\")\n",
        "cm=confusion_matrix(test_y,y_pred)\n",
        "for i in range(3):\n",
        "  print(\"Accuracy for class \", i+1)\n",
        "  print((cm.diagonal()/cm.sum(axis=1))[i] *100) #Individual Class Accuracy\n",
        "print(\"Number of correctly predicted class labels are:\",np.sum(y_pred==test_y))\n",
        "print(\"Total number of class labels are:\",len(test_y))\n",
        "print(\"Overall Accuracy is:\",np.sum(y_pred==test_y)/len(test_y) *100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cross validation \n",
            "Individual class accuracy:\n",
            "Accuracy for class  1\n",
            "100.0\n",
            "Accuracy for class  2\n",
            "100.0\n",
            "Accuracy for class  3\n",
            "100.0\n",
            "Number of correctly predicted class labels are: 144\n",
            "Total number of class labels are: 144\n",
            "Overall Accuracy is: 100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dErg1NtIJqa",
        "outputId": "a9db910e-caf9-4de5-ffe5-55e46893b53b"
      },
      "source": [
        "j=1\n",
        "Xs, ys = shuffle(X, y,random_state=0)\n",
        "print(\"10 Fold Cross validation is as follows\")\n",
        "kf = KFold(n_splits=10)\n",
        "for train,test in kf.split(Xs):\n",
        "  print(\"For Fold \",j)\n",
        "  j += 1\n",
        "  train_data_5_xx = Xs[train]\n",
        "  train_data_5_yy = ys[train]\n",
        "  test_data5_xx = Xs[test]\n",
        "  test_data5_yy = ys[test]\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "  warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "  y_pred=np.argmax(model_10_instance(train_data_5_xx,train_data_5_yy).predict(test_data5_xx), axis=-1)\n",
        "  cm=confusion_matrix(test_data5_yy,y_pred)\n",
        "  print(\"Confusion matrix is\")\n",
        "  print(cm)\n",
        "  print(\"Individual class accuracy is as follows\")\n",
        "  for i in range(3):\n",
        "    print(\"Accuracy for class \", i+1)\n",
        "    print((cm.diagonal()/cm.sum(axis=1))[i] *100)\n",
        "\n",
        "  print(\"Number of correctly predicted class labels are: \",np.sum(y_pred==test_data5_yy))\n",
        "  print(\"Total number of class labels are:               \",len(test_data5_yy))\n",
        "  print(\"Overall Accuracy is:                            \",np.sum(y_pred==test_data5_yy)/len(test_data5_yy) *100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 Fold Cross validation is as follows: \n",
            "\n",
            "For Fold  1\n",
            "Confusion matrix is\n",
            "[[17  1  0]\n",
            " [ 0 21  0]\n",
            " [ 0  0 33]]\n",
            "Individual class accuracy is as follows\n",
            "Accuracy for class  1\n",
            "94.44444444444444\n",
            "\n",
            "Accuracy for class  2\n",
            "100.0\n",
            "\n",
            "Accuracy for class  3\n",
            "100.0\n",
            "\n",
            "Number of correctly predicted class labels are:  71\n",
            "Total number of class labels are:                72\n",
            "Overall Accuracy is:                             98.61111111111111\n",
            "\n",
            "For Fold  2\n",
            "Confusion matrix is\n",
            "[[24  0  0]\n",
            " [ 0 33  0]\n",
            " [ 0  0 15]]\n",
            "Individual class accuracy is as follows\n",
            "Accuracy for class  1\n",
            "100.0\n",
            "\n",
            "Accuracy for class  2\n",
            "100.0\n",
            "\n",
            "Accuracy for class  3\n",
            "100.0\n",
            "\n",
            "Number of correctly predicted class labels are:  72\n",
            "Total number of class labels are:                72\n",
            "Overall Accuracy is:                             100.0\n",
            "\n",
            "For Fold  3\n",
            "Confusion matrix is\n",
            "[[19  0  0]\n",
            " [ 0 31  0]\n",
            " [ 0  0 22]]\n",
            "Individual class accuracy is as follows\n",
            "Accuracy for class  1\n",
            "100.0\n",
            "\n",
            "Accuracy for class  2\n",
            "100.0\n",
            "\n",
            "Accuracy for class  3\n",
            "100.0\n",
            "\n",
            "Number of correctly predicted class labels are:  72\n",
            "Total number of class labels are:                72\n",
            "Overall Accuracy is:                             100.0\n",
            "\n",
            "For Fold  4\n",
            "Confusion matrix is\n",
            "[[31  0  0]\n",
            " [ 0 23  0]\n",
            " [ 0  0 18]]\n",
            "Individual class accuracy is as follows\n",
            "Accuracy for class  1\n",
            "100.0\n",
            "\n",
            "Accuracy for class  2\n",
            "100.0\n",
            "\n",
            "Accuracy for class  3\n",
            "100.0\n",
            "\n",
            "Number of correctly predicted class labels are:  72\n",
            "Total number of class labels are:                72\n",
            "Overall Accuracy is:                             100.0\n",
            "\n",
            "For Fold  5\n",
            "Confusion matrix is\n",
            "[[22  0  0]\n",
            " [ 0 28  0]\n",
            " [ 1  0 21]]\n",
            "Individual class accuracy is as follows\n",
            "Accuracy for class  1\n",
            "100.0\n",
            "\n",
            "Accuracy for class  2\n",
            "100.0\n",
            "\n",
            "Accuracy for class  3\n",
            "95.45454545454545\n",
            "\n",
            "Number of correctly predicted class labels are:  71\n",
            "Total number of class labels are:                72\n",
            "Overall Accuracy is:                             98.61111111111111\n",
            "\n",
            "For Fold  6\n",
            "Confusion matrix is\n",
            "[[21  0  0]\n",
            " [ 0 28  0]\n",
            " [ 0  0 23]]\n",
            "Individual class accuracy is as follows\n",
            "Accuracy for class  1\n",
            "100.0\n",
            "\n",
            "Accuracy for class  2\n",
            "100.0\n",
            "\n",
            "Accuracy for class  3\n",
            "100.0\n",
            "\n",
            "Number of correctly predicted class labels are:  72\n",
            "Total number of class labels are:                72\n",
            "Overall Accuracy is:                             100.0\n",
            "\n",
            "For Fold  7\n",
            "Confusion matrix is\n",
            "[[23  0  0]\n",
            " [ 1 17  0]\n",
            " [ 0  0 31]]\n",
            "Individual class accuracy is as follows\n",
            "Accuracy for class  1\n",
            "100.0\n",
            "\n",
            "Accuracy for class  2\n",
            "94.44444444444444\n",
            "\n",
            "Accuracy for class  3\n",
            "100.0\n",
            "\n",
            "Number of correctly predicted class labels are:  71\n",
            "Total number of class labels are:                72\n",
            "Overall Accuracy is:                             98.61111111111111\n",
            "\n",
            "For Fold  8\n",
            "Confusion matrix is\n",
            "[[28  0  0]\n",
            " [ 0 21  0]\n",
            " [ 0  0 23]]\n",
            "Individual class accuracy is as follows\n",
            "Accuracy for class  1\n",
            "100.0\n",
            "\n",
            "Accuracy for class  2\n",
            "100.0\n",
            "\n",
            "Accuracy for class  3\n",
            "100.0\n",
            "\n",
            "Number of correctly predicted class labels are:  72\n",
            "Total number of class labels are:                72\n",
            "Overall Accuracy is:                             100.0\n",
            "\n",
            "For Fold  9\n",
            "Confusion matrix is\n",
            "[[28  0  0]\n",
            " [ 0 25  0]\n",
            " [ 0  0 19]]\n",
            "Individual class accuracy is as follows\n",
            "Accuracy for class  1\n",
            "100.0\n",
            "\n",
            "Accuracy for class  2\n",
            "100.0\n",
            "\n",
            "Accuracy for class  3\n",
            "100.0\n",
            "\n",
            "Number of correctly predicted class labels are:  72\n",
            "Total number of class labels are:                72\n",
            "Overall Accuracy is:                             100.0\n",
            "\n",
            "For Fold  10\n",
            "Confusion matrix is\n",
            "[[23  0  0]\n",
            " [ 0 24  0]\n",
            " [ 0  0 25]]\n",
            "Individual class accuracy is as follows\n",
            "Accuracy for class  1\n",
            "100.0\n",
            "\n",
            "Accuracy for class  2\n",
            "100.0\n",
            "\n",
            "Accuracy for class  3\n",
            "100.0\n",
            "\n",
            "Number of correctly predicted class labels are:  72\n",
            "Total number of class labels are:                72\n",
            "Overall Accuracy is:                             100.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}